{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yachay.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yachay is an open-source machine learning community with decades worth of natural language data from media, the dark web, legal proceedings, and government publications. They have cleaned and annotated the data, and created a geolocation detection tool. They are looking for developers interested in contributing and improving on the project. We are given a dataset of tweets, and another dataset of coordinates, upon which we will create a neural network to predict coordinates from text. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "df_main = pd.read_csv('data\\Main_Dataset.csv', parse_dates=['timestamp'], index_col=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by timestamp\n",
    "df_main.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at dataset\n",
    "df_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.index.is_monotonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at column information\n",
    "df_main.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for missing values\n",
    "df_main.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for duplicates\n",
    "df_main.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data with missing index\n",
    "df_main.index.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of data with missing index\n",
    "df_main.index.isna().sum() / len(df_main) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at missing data with missing index\n",
    "df_main[df_main.index.isna()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the main dataset is fairly clean. We loaded the data as a timeseries, and parsed the dates. This dataframe contains most of the features we need to train a our model. The data that is missing is limited to timestamps, while the other columns of this data is present. As the missing data represents 2% of the entire dataset, and becase we are unable to impute the timestamps, we will drop these rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cluster data\n",
    "df_cl = pd.read_csv('data/Clusters_Coordinates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at dataset\n",
    "df_cl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at column info\n",
    "df_cl.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for missing values\n",
    "df_cl.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster coordinates dataframe contains the cluster id as well as the latitutde and longitude data. This dataframe is clean with no missing values. We will merge the two dataframes before conducting EDA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual of data before feature engineering\n",
    "df_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making timestamp features\n",
    "def make_features(data):\n",
    "    data['year'] = data.index.year\n",
    "    data['month'] = data.index.month\n",
    "    data['week'] = data.index.isocalendar().week\n",
    "    data['day'] = data.index.day\n",
    "    data['day_of_week'] = data.index.day_of_week \n",
    "    data['day_of_year'] = data.index.day_of_year\n",
    "    data['hour'] = data.index.hour \n",
    "    data['minute'] = data.index.minute \n",
    "    data['second'] = data.index.second\n",
    "    \n",
    "make_features(df_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new features added\n",
    "df_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge main and cluster coordinates\n",
    "df = df_main.merge(df_cl, on='cluster_id', sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new merged dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('processed data/df.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merged the datasets on cluster id. We then dropped all rows with the missing timestamp data. We are left with a total of close to 600,000 rows of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique users\n",
    "df.user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique clusters\n",
    "df.cluster_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique latitudes\n",
    "df.lat.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique longitudes\n",
    "df.lng.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skew of data\n",
    "df.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation of data\n",
    "px.imshow(df.corr(), text_auto=True, aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributions of columns\n",
    "columns = ['month', 'week', 'day', 'day_of_week', 'day_of_year', 'hour', 'minute', 'second']\n",
    "for column in columns:\n",
    "    px.histogram(df[column], title='Distribution of '+ str.upper(column).replace('_', ' '), labels={'value': str(column).replace('_', ' ')}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df.user_id.value_counts(), title='Distribution of User Id\\'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df.cluster_id.value_counts(), title='Distribution of Cluster Id\\'s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the data is distributed towards two months out of the year: near the beginning and more near the end. We can see this trend in the monthly and year of the week timeframes. Along the daily timeframe, the data is uniformly distributed up until the last week of the month, at which point it declines to roughly half the mean of the month. The day of the week is distributed normally, so there are no differences seen during the week versus the weekned. The distribution in the time of day is mostly uniform, except for the gradual decline in the middle of the day immediately followed by the gradual increase back to uniform distribution. The lower timeframes of minutes and seconds are uniformly distributed. \n",
    "\n",
    "User id and cluster id are right skewed. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> maps in maps notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is concentrated in North America, with most coordinates in the US and Mexico. We see most tweets appear in popular cities in the east coast and the west coast, with clusters in New York, California, and Florida. We see a few tweets originate from Alaska, Canada, Hawaii, and the Caribbean. \n",
    "\n",
    "The heatmap further illustrates the distribution of tweets in the east coast of the US."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLP tools we will be using are from Huggingface. We will test several different models to determine which one would work best with our dataset. We narrowed the options to BERT base, BERT multilingual, and XLM Roberta. Preprocessing will be similar among the three models, while the multilingual and XLM are optimized for english nd many other languages. We anticipate the multi language models to perform better than the base model, as our text data contains many different languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking through tweets\n",
    "tweets = df.text.tolist()\n",
    "tweets[10:21]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess data for modelling \n",
    "def preprocess_base(df, max_sample, batch_size=200):\n",
    "    max_sample_size = max_sample # set the max sample size\n",
    "\n",
    "    # preprocessing and BERT\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    ids_list_df = []\n",
    "    attention_mask_list_df = []\n",
    "\n",
    "    max_length = 512\n",
    "\n",
    "    for input_text in df.iloc[:max_sample_size]['text']:\n",
    "        ids = tokenizer.encode(input_text.lower(), add_special_tokens=True, truncation=True, max_length=max_length)\n",
    "        padded = np.array(ids + [0]*(max_length - len(ids)))\n",
    "        attention_mask = np.where(padded != 0, 1, 0)\n",
    "        ids_list_df.append(padded)\n",
    "        attention_mask_list_df.append(attention_mask)\n",
    "    \n",
    "    # get embeddings \n",
    "    config = transformers.BertConfig.from_pretrained('bert-base-uncased')\n",
    "    model = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    batch_size = batch_size    # typically the batch size is equal to 100 but we can set it to lower values to lower the memory requirements\n",
    "\n",
    "    embeddings_df = []\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # will use cpu unless cuda is available\n",
    "    print(f'Using the {device} device.')\n",
    "    model.to(device)\n",
    "\n",
    "    for i in tqdm(range(len(ids_list_df) // batch_size)):\n",
    "        \n",
    "        ids_batch_df = torch.LongTensor(ids_list_df[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "        attention_mask_batch_df = torch.LongTensor(attention_mask_list_df[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            batch_embeddings = model(ids_batch_df, attention_mask=attention_mask_batch_df)\n",
    "\n",
    "        embeddings_df.append(batch_embeddings[0][:,0,:].detach().cpu().numpy())\n",
    "\n",
    "    X = np.concatenate(embeddings_df)  # create features\n",
    "    y = df.iloc[:max_sample_size][['lat', 'lng']] # create target with matching length as features\n",
    "\n",
    "    print(X.shape)  # illustrate matching length\n",
    "    print(y.shape)   # illustrate matching length\n",
    "\n",
    "    return X, y  # return the processed features and target dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing training data \n",
    "#X_base, y_base = preprocess_base(df[591408:], len(df[591408:]), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(\"processed data/X_base.csv\", X_base, delimiter=\",\")\n",
    "# y_base.to_csv('processed data/y_base.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess data for modelling \n",
    "def preprocess_multi(df, max_sample, batch_size=200):\n",
    "    max_sample_size = max_sample # set the max sample size\n",
    "\n",
    "    # preprocessing and BERT\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "    ids_list_df = []\n",
    "    attention_mask_list_df = []\n",
    "\n",
    "    max_length = 512\n",
    "\n",
    "    for input_text in df.iloc[:max_sample_size]['text']:\n",
    "        ids = tokenizer.encode(input_text.lower(), add_special_tokens=True, truncation=True, max_length=max_length)\n",
    "        padded = np.array(ids + [0]*(max_length - len(ids)))\n",
    "        attention_mask = np.where(padded != 0, 1, 0)\n",
    "        ids_list_df.append(padded)\n",
    "        attention_mask_list_df.append(attention_mask)\n",
    "    \n",
    "    # get embeddings \n",
    "    config = transformers.BertConfig.from_pretrained('bert-base-multilingual-uncased')\n",
    "    model = transformers.BertModel.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "    batch_size = batch_size    # typically the batch size is equal to 100 but we can set it to lower values to lower the memory requirements\n",
    "\n",
    "    embeddings_df = []\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # will use cpu unless cuda is available\n",
    "    print(f'Using the {device} device.')\n",
    "    model.to(device)\n",
    "\n",
    "    for i in tqdm(range(len(ids_list_df) // batch_size)):\n",
    "        \n",
    "        ids_batch_df = torch.LongTensor(ids_list_df[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "        attention_mask_batch_df = torch.LongTensor(attention_mask_list_df[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            batch_embeddings = model(ids_batch_df, attention_mask=attention_mask_batch_df)\n",
    "\n",
    "        embeddings_df.append(batch_embeddings[0][:,0,:].detach().cpu().numpy())\n",
    "\n",
    "    X = np.concatenate(embeddings_df)  # create features\n",
    "    y = df.iloc[:max_sample_size][['lat', 'lng']] # create target with matching length as features\n",
    "\n",
    "    print(X.shape)  # illustrate matching length\n",
    "    print(y.shape)   # illustrate matching length\n",
    "    \n",
    "    return X, y  # return the features and target dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing training data \n",
    "# X_multi, y_multi = preprocess_multi(df, 591412, 296)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_multi = pd.DataFrame(X_multi)\n",
    "# X_multi.to_csv('/notebooks/X_base.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(\"processed data/X_multi.csv\", X_base, delimiter=\",\")\n",
    "# y_multi.to_csv('processed data/y_multi.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess data for modelling \n",
    "def preprocess_xlm(df, max_sample, batch_size=200):\n",
    "    max_sample_size = max_sample # set the max sample size\n",
    "\n",
    "    # preprocessing and XLM-RoBERTa\n",
    "    tokenizer = transformers.XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-large')\n",
    "\n",
    "    ids_list_df = []\n",
    "    attention_mask_list_df = []\n",
    "\n",
    "    max_length = 512\n",
    "\n",
    "    for input_text in df.iloc[:max_sample_size]['text']:\n",
    "        ids = tokenizer.encode(input_text.lower(), add_special_tokens=True, truncation=True, max_length=max_length)\n",
    "        padded = np.array(ids + [0]*(max_length - len(ids)))\n",
    "        attention_mask = np.where(padded != 0, 1, 0)\n",
    "        ids_list_df.append(padded)\n",
    "        attention_mask_list_df.append(attention_mask)\n",
    "    \n",
    "    # get embeddings \n",
    "    config = transformers.XLMRobertaConfig.from_pretrained('xlm-roberta-large')\n",
    "    model = transformers.XLMRobertaModel.from_pretrained('xlm-roberta-large')\n",
    "\n",
    "    batch_size = batch_size    # typically the batch size is equal to 100 but we can set it to lower values to lower the memory requirements\n",
    "\n",
    "    embeddings_df = []\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # will use cpu unless cuda is available\n",
    "    print(f'Using the {device} device.')\n",
    "    model.to(device)\n",
    "\n",
    "    for i in tqdm(range(len(ids_list_df) // batch_size)):\n",
    "        \n",
    "        ids_batch_df = torch.LongTensor(ids_list_df[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "        attention_mask_batch_df = torch.LongTensor(attention_mask_list_df[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            batch_embeddings = model(ids_batch_df, attention_mask=attention_mask_batch_df)\n",
    "\n",
    "        embeddings_df.append(batch_embeddings[0][:,0,:].detach().cpu().numpy())\n",
    "\n",
    "    X = np.concatenate(embeddings_df)  # create features\n",
    "    y = df.iloc[:max_sample_size][['lat', 'lng']] # create target with matching length as features\n",
    "\n",
    "    print(X.shape)  # illustrate matching length\n",
    "    print(y.shape)   # illustrate matching length\n",
    "    \n",
    "    return X, y  # return the features and target dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocessed the data into three different sets of training and test data. We wil run our models on each of the three sets to determine which NLP model worked best for our data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample of larger dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With limited resources, we can not work with the entire dataset of half a million rows. Our approach is to take a manageable sample of the data to work with. We will save that dataset, to reduce computation times. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Need to get a random sample of dataframe before tokenization, to ensure we get a much closer representation of the distribution of coordinates. current coordinates of non-random dataset is Los Angeles, CA.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"processed data/X_xlm_1000.csv\", X_xlm, delimiter=\",\")\n",
    "#y_xlm.to_csv('processed data/y_xlm_1000.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Processed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_base = pd.read_csv('inputs/X_base.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_base.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilingual Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_multi = pd.read_csv('inputs/X_multi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_multi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_xlm = pd.read_csv('inputs/X_xlm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_xlm.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('inputs/y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=19) # split 20% of data to make validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# haversine distance loss\n",
    "RADIUS_KM = 6378.1\n",
    "\n",
    "def degrees_to_radians(deg):\n",
    "    pi_on_180 = 0.017453292519943295\n",
    "    return deg * pi_on_180\n",
    "\n",
    "def loss_haversine(observation, prediction):    \n",
    "    obv_rad = tf.map_fn(degrees_to_radians, observation)\n",
    "    prev_rad = tf.map_fn(degrees_to_radians, prediction)\n",
    "\n",
    "    dlon_dlat = obv_rad - prev_rad \n",
    "    v = dlon_dlat / 2\n",
    "    v = tf.sin(v)\n",
    "    v = v**2\n",
    "\n",
    "    a = v[:,1] + tf.cos(obv_rad[:,1]) * tf.cos(prev_rad[:,1]) * v[:,0] \n",
    "\n",
    "    c = tf.sqrt(a)\n",
    "    c = 2* tf.math.asin(c)\n",
    "    c = c*RADIUS_KM\n",
    "    final = tf.reduce_sum(c)\n",
    "\n",
    "    #if you're interested in having MAE with the haversine distance in KM\n",
    "    #uncomment the following line\n",
    "    final = final/tf.dtypes.cast(tf.shape(observation)[0], dtype= tf.float32)\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(19)\n",
    "optimizer = Adam(learning_rate=.0001)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=5, min_lr=0.0000001)\n",
    "\n",
    "# define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(4000, activation='relu', input_dim=(X_train.shape[1])))\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "model.add(Dense(2)) # output layer with 2 units for latitude and longitude\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_haversine, metrics=['mse'])\n",
    "\n",
    "# train the model\n",
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(X_train, y_train, epochs=1, batch_size=32, validation_split=0.10, callbacks=[callback, reduce_lr], use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(19)\n",
    "optimizer = Adam(learning_rate=.0001)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=5, min_lr=0.0000001)\n",
    "\n",
    "# define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(8000, activation='relu', input_dim=(X_train.shape[1])))\n",
    "model.add(Dense(4000, activation='relu'))\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(2)) # output layer with 2 units for latitude and longitude\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_haversine, metrics=['mse'])\n",
    "\n",
    "# train the model\n",
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.10, callbacks=[callback, reduce_lr], use_multiprocessing=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, MaxPooling1D, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Reshape((X_train.shape[1], 1), input_shape=(X_train.shape[1],)))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(2))\n",
    "model.compile(optimizer=optimizer, loss=loss_haversine, metrics=['mse'])\n",
    "history = model.fit(X_train, y_train, epochs=1, batch_size=32, validation_split=0.10, callbacks=[callback, reduce_lr])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(19)\n",
    "optimizer = Adam(learning_rate=.0001)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=5, min_lr=0.0000001)\n",
    "\n",
    "# define the model architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(10, input_shape=(X_train.shape[1], 1), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(5, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2)) # output layer with 2 units for latitude and longitude\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_haversine, metrics=['mse'])\n",
    "\n",
    "# train the model\n",
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1)), y_train, epochs=1, batch_size=32, validation_split=0.10, callbacks=[callback, reduce_lr], use_multiprocessing=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=128, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(2))\n",
    "model.compile(optimizer=optimizer, loss=loss_haversine, metrics=['mse'])\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.10, callbacks=[callback, reduce_lr])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, GRU, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# set random seed for reproducibility\n",
    "tf.random.set_seed(19)\n",
    "\n",
    "# define the inputs\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "# GRU layer with 64 units and default activation function (tanh)\n",
    "gru = GRU(64)(inputs)\n",
    "\n",
    "# dense layers with relu activation function\n",
    "dense1 = Dense(128, activation='relu')(gru)\n",
    "dense2 = Dense(64, activation='relu')(dense1)\n",
    "\n",
    "# output layer with 2 units for latitude and longitude\n",
    "outputs = Dense(2)(dense2)\n",
    "\n",
    "# define the model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# compile the model\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss=loss_haversine, metrics=['mse'])\n",
    "\n",
    "# train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1, callbacks=[callback, reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation on test set\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X test predictions\n",
    "preds = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the model history to a pandas DataFrame\n",
    "df_his = pd.DataFrame(history.history)\n",
    "\n",
    "# Create separate figures for loss and accuracy\n",
    "fig_loss = px.line(df_his, x=df_his.index, y=['loss', 'val_loss'], labels={'value': 'Loss', 'index': 'Epoch'}, title='Model Loss')\n",
    "fig_acc = px.line(df_his, x=df_his.index, y=['mse', 'val_mse'], labels={'value': 'MSE', 'index': 'Epoch'}, title='Model MSE')\n",
    "fig_lr = px.line(df_his, x=df_his.index, y='lr', labels={'value': 'Learning Rate', 'index': 'Epoch'}, title='Model Learning Rate', log_y=True)\n",
    "\n",
    "# Show the figures\n",
    "fig_loss.show()\n",
    "fig_acc.show()\n",
    "fig_lr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame(preds, columns=['lat_p', 'lng_p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = pd.concat([y_df, preds_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haversine distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsas = [34.020789, -118.411907]\n",
    "paris = [34.087627, -118.664711]\n",
    "bsas_in_radians = [radians(_) for _ in bsas]\n",
    "paris_in_radians = [radians(_) for _ in paris]\n",
    "result = haversine_distances([bsas_in_radians, paris_in_radians])\n",
    "result * 6371000/1000  # multiply by Earth radius to get kilometers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert test set coordinates to radians   \n",
    "y_test_rad = y_test * (math.pi/180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert prediction coordinates to radians\n",
    "preds_rad = preds * (math.pi/180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate distance\n",
    "distances = haversine_distances(y_test_rad, preds_rad)[0]\n",
    "distances_km = distances * (6371000/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(distances_km, title='Distances Between Actual and Prediction', labels={'value': 'Distance (Km)'}, template='presentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(distances_km, title='Distribution of Distances', labels={'value': 'Distance (Km)'}, template='plotly_white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment analysis\n",
    "# from transformers import pipeline\n",
    "# sent = pipeline('sentiment-analysis', model='cardiffnlp/twitter-xlm-roberta-base-sentiment')(df.text.values.tolist())\n",
    "# sent = pd.DataFrame(sent, columns=['label', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentiment dataframe\n",
    "sent = pd.read_csv('inputs/sent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns \n",
    "sent.rename(columns={'label': 'sent', 'score': 'sent_score'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lok at sentiment dataframe\n",
    "sent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment labels\n",
    "sent.sent.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment values\n",
    "sent.sent.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(sent.sent, color=sent.sent,  title='Tweet Sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoder column transformer\n",
    "sent_encoder = make_column_transformer((OneHotEncoder(), ['sent']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode sentiment\n",
    "sentiment_array = sent_encoder.fit_transform(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment array\n",
    "sentiment_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sentiment_array).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_dummies = pd.get_dummies(sent.sent)\n",
    "sent_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained('ivanlau/language-detection-fine-tuned-on-xlm-roberta-base')\n",
    "# model = AutoModelForSequenceClassification.from_pretrained('ivanlau/language-detection-fine-tuned-on-xlm-roberta-base')\n",
    "\n",
    "# Set up the pipeline\n",
    "# classifier = pipeline('text-classification', model=model, tokenizer=tokenizer, device_map='auto')\n",
    "\n",
    "# Example usage\n",
    "# result = []\n",
    "# for text in tqdm(df.text.values.tolist()):\n",
    "    # result.append(classifier(text))\n",
    "# df_lan = pd.DataFrame(result, columns=['label', 'score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load languages dataframe\n",
    "lan = pd.read_csv('inputs/lan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns \n",
    "lan.rename(columns={'label': 'language', 'score': 'lang_score'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts of the different languages\n",
    "lan_counts = lan.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual of different language counts\n",
    "px.bar(lan_counts, color=lan_counts.index, title='Tweet Languages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoder column transformer\n",
    "lan_encoder = make_column_transformer((OneHotEncoder(), ['language']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode languages\n",
    "lan_array = lan_encoder.fit_transform(lan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lan_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lan_array).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoder column transformer\n",
    "lan_encoder2 = make_column_transformer((LabelEncoder(), ['language']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dummies = pd.get_dummies(lan.language)\n",
    "lang_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "lang_labels = encoder.fit_transform(pd.DataFrame(lan.language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER tokenizer\n",
    "# token_classifier = pipeline(model=\"Abderrahim2/bert-finetuned-Location\")\n",
    "\n",
    "# tokens = token_classifier(df_rand.text.to_list())\n",
    "# tokens= pd.DataFrame(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ner dataframe\n",
    "ner = pd.read_csv('inputs/ner.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at ner dataframe\n",
    "ner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_count = ner.entity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(ner_count, color=ner_count.index, title='Tweet Locations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoder column transformer\n",
    "ner_encoder = make_column_transformer((OneHotEncoder(), ['entity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode languages\n",
    "ner_array = ner_encoder.fit_transform(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoder column transformer\n",
    "ner_encoder2 = make_column_transformer((LabelEncoder(), ['entity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dummies = pd.get_dummies(ner.entity)\n",
    "ner_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "ner_labels = encoder.fit_transform(pd.DataFrame(ner.entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic classifier\n",
    "# topic_classifier = pipeline(model=\"jonaskoenig/topic_classification_04\")\n",
    "\n",
    "# result1 = []\n",
    "# for text in tqdm(df.text.values.tolist()):\n",
    "#    result1.append(topic_classifier(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = pd.read_csv('inputs/topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics.columns=['topic', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics.topic.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_count= topics.topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(topics_count, color=topics_count.index, title='Tweet Topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoder column transformer\n",
    "topics_encoder = make_column_transformer((OneHotEncoder(), ['topic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_array = topics_encoder.fit_transform(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_dummies = pd.get_dummies(topics.topic)\n",
    "topics_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "topic_labels = encoder.fit_transform(pd.DataFrame(topics.topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
